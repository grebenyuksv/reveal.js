Now let's see how we solved those.
I'll give you a toolbox of <b>seven</b> instruments. Most of this, we learned the hard way.
Most of these, things we didn't do. In my opinion we succeeded (well, yes, in my opinion, we
succeeded üôÇ) because a) we still did many things well b) our mistakes were compensated by several
factors:
1. some people voluntarily worked overtime;
2. some people went far beyond their tech stack and business logic;
3. we were somewhat lucky. All possible issues just didn't happen at the same time. üôÇ

---------------------------------------------------------------------

Tool 1. Increase urgency.
<br />
It's <i>natural</i> that tests are less important than real users.
We have to very explicitly define who does what by when when tests fail. Don't just rely on culture.
This is <i>good culture</i>.
Our goal requires <i>more</i>.

---------------------------------------------------------------------

There is a widespread belief: tests are optional because they are unstable.
The real causality is in the opposite direction.
We must instruct people to pause other work for investigation.
We must compensate people if it's outside of working hours. Especially if the company operates in
distant time zones.
If that is unsustainable: we turn everything off, we analyze what we are doing wrong, and we turn it
on again.
If we have no ideas, we either reduce the amount of tests or we conclude that this is not for us and
we shut down the entire initiative.
<br />
So this one I called increase the urgency.

---------------------------------------------------------------------

Tool 2. Analyze every failure.
I will show you two terrible slides. You don't have to read them, just grasp
the structure. This is
just a helicopter view of
what it looked like.

---------------------------------------------------------------------

Analyze:
<br />
- [horizontal gesture] Which part of the product?
<br />
- [vertical gesture] Which part of the stack?
<br />
- How many failures?
<br />
- Who?
<br />
- How to prevent it in the future?
<br />

I admit this idea of analyzing every failure is fairly obvious. What is not obvious and what scared
me in the very beginning is it might seem that it's not worth trying because there might be too many
errors.

It appeared to be alright. In our reality, for a combination of reasons, it was a somewhat full-time
job, but it still fit into one brain.

If we do this, we can get to... [next slide]

---------------------------------------------------------------------

... this. This gives not only actionability but also the scope (and thus the cost) of the next
iteration. We can see in
which order to do the improvements and what
kind of improvements there are. Be it's technical or people work.
This slide says:
<br />
- 13 failures in Q2 wouldn't have happened if the checks were mandatory for pull requests (which was
not the case)
- 10 can be fixed by guiding people how to write deterministic tests
- 11 were one-off glitches which we have to live with because it brings unreasonable
overhead to fix them. That happens too.
<br />
Note: identifying the who does not by any means violate the blameless culture (at least because
there's <i>just no blame</i> in saying, e.g. We weren't able to fetch Docker images because
Microsoft registry was down so the FE Platform team should build a cache).
We can think of it like this: we are measuring processes, not people. We don't have to measure
people because we just assume good intention.
<br />
So this was about analyzing every failure.
<br />
And it does not work without tool 3.

---------------------------------------------------------------------

Tool 3. Please, make accountable people from the entire technical stack, I can't stress this more.
<br />
We didn‚Äôt do it ‚Äî FE engineers led the project ‚Äî and we find ourselves in situations where BE people
whom we needed to do investigations and code improvements had other priorities.
<br />
I will illustrate this with one example of a specific problem that we still have to be honest with
you.
We observed specific tests failing randomly. The buttons just weren't showing up where they should.
When we talked to the authors of the tests, they couldn't explain it. But they said the buttons
should be there because that was an A/B test that got scaled long ago.
that the front-end platform team analyzed the reports closer and saw that the back-end was returning
to the front-end. The response is saying that that A/B test was not scaled.
We talked to the experimentation team. They couldn't explain it because that code was constructed by
someone else before the experimentation team was even created.
Then we asked the backend platform to help us with the investigation but it was not in their
priorities.
So now we just live with it. The problem is still there, but because it doesn't strike <i>very</i>
often. It's not a high priority for the owners of those mechanisms.
<br />
I hope this example illustrates why it's not enough to ask FE engineers to build stable tests.
We need to make a stable product and that could only be done by people from the entire
stack.

---------------------------------------------------------------------

Tool 4. Educate.

1. Teach BE engineers to read the test reports.
2. Teach FE engineers to analyze server logs.

---------------------------------------------------------------------

Tool 5. Assign senior engineers.
I realize it's hard to challenge the idea of delegating constructing the tests to more junior
engineers. However, it's a hard job to debug the failure when looking at a test report.

Full disclosure: we didn‚Äôt do it. In many cases, we still had to call senior engineers - and
we lost some stability because that took longer.

---------------------------------------------------------------------

Tool 6. Increase visibility.
Try to improve the developer experience of your engineers during the times when the stability
requirements are not met. This is one thing we did. It says the test failed, but it also mentions
the known issues. There is one known issue. This is how the symptoms look like. The workaround is to
rerun the job.

This is an attempt to tolerate instability. Yes, these failures happened. But if we give people good
developer experience, these failures might have a very limited impact on their <i>perception</i> of
the stability.

First: it's hard to calculate the required stability because we're trying to predict human
perception, which is different for everyone across the company. Even if you're in the green zone,
there are incidents where people would
say "I didn't run E2E tests because I know they're flaky.‚Äù

Second: we anyway need a way to tolerate the instabilities because every issue significantly drops
the stability for the short time window.

Fun fact: when we asked people their perception of the stability, it appeared higher than the
reality.

---------------------------------------------------------------------

Tool 7. Start small.
We started with three teams right away. It took us time to reach stability because there were just
too many problems in the beginning.
<br /><br />[reveal fragment]<br /><br />
And never go too big :) More tests of this kind bring diminishing returns.

---------------------------------------------------------------------

Conclusions. This is the opposite of simplicity. It's an expensive tool due to a lot of
organizational work. On every step, we should validate
the returns on our investment. This is not the only way to test applications.

If we do go this path, we should conscientiously use those tools that I listed. Of
course there is a chance it works
out if we just put a bunch of smart people in the room ‚Äî but that's <i>hope</i>.
Hope is not a good plan.
